{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tutsilianna/Automatic_Text_Processing_and_Image_Processing/blob/main/Lexical%20Resources%20in%20Automatic%20Text%20Processing/Task_4_%7C_Lexical_Resources_in_Automatic_Text_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLYpSPDlW9LC"
      },
      "source": [
        "## Download everything we need\n",
        "\n",
        "Ne need to download WordNet by means of NLTK.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhTQ6EFXZ5R9",
        "outputId": "db11176a-3f6b-41e4-a37d-7d306d0de598"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIQ4rEytvwLZ"
      },
      "source": [
        "## Prepare the data\n",
        "\n",
        "We import the data from a prepared text file. The file contains the set of word pairs (just nouns), for which expert similarity estimates are known.\n",
        "\n",
        "We make an associative array of \"word pair - similarity estimate\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sTFACx3dAk8"
      },
      "source": [
        "with open(\"wordsim_similarity_goldstandard.txt\", encoding=\"utf-8\") as rf:\n",
        "  triples = [line.strip().split(\"\\t\") for line in rf.readlines()]\n",
        "  score_map = {tuple(triple[:2]): float(triple[2]) for triple in triples}"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRm9S90NBUAs"
      },
      "source": [
        "Note, that we took just expert similarity estimates from the original file and for nouns only. The original set is available [here](http://alfonseca.org/pubs/ws353simrel.tar.gz)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96B0OtKrvtaG"
      },
      "source": [
        "Let's have a look at similarity measure examples.\n",
        "\n",
        "Some words can have several different meanings in WordNet. Here -- just as an example -- we will select the first one that comes across, but then we will work with them differently.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iXamIiZgf-O",
        "outputId": "e9e6bd7b-4d8d-4379-b99e-5367867f16fc"
      },
      "source": [
        "for w1, w2 in list(score_map)[:2]:\n",
        "\n",
        "  print(\"\\nWords: %s-%s\\nGround truth score: %.2f\" % (w1, w2, score_map[(w1, w2)]))\n",
        "\n",
        "  ss1 = wn.synset(w1 + \".n.01\")\n",
        "  ss2 = wn.synset(w2 + \".n.01\")\n",
        "\n",
        "  print(\"\\nPath: %.3f\" % ss1.path_similarity(ss2), end=\" \")\n",
        "  print(\"\\nwup: %.3f\" % ss1.wup_similarity(ss2), end=\" \")\n",
        "  print(\"\\nshortest_path: %.3f\" % ss1.shortest_path_distance(ss2))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Words: tiger-cat\n",
            "Ground truth score: 7.35\n",
            "\n",
            "Path: 0.091 \n",
            "wup: 0.545 \n",
            "shortest_path: 10.000\n",
            "\n",
            "Words: tiger-tiger\n",
            "Ground truth score: 10.00\n",
            "\n",
            "Path: 1.000 \n",
            "wup: 0.750 \n",
            "shortest_path: 0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHM7tCb0vqNp"
      },
      "source": [
        "Compute several similarity measures for all word pairs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe7Nuglqgnd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85dd9ca9-f6c6-4e53-b0a0-ecb405b9ebbb"
      },
      "source": [
        "from itertools import product\n",
        "\n",
        "list_pairs = list(score_map)\n",
        "wup_list, true_list, path_list = [], [], []\n",
        "\n",
        "# для всех пар\n",
        "for w1, w2 in list_pairs:\n",
        "\n",
        "  try:\n",
        "    all_w1 = wn.synsets(w1, pos=\"n\")\n",
        "    all_w2 = wn.synsets(w2, pos=\"n\")\n",
        "\n",
        "    # we add metrics of interest and expert reviews\n",
        "    wup = max([item1.wup_similarity(item2) \\\n",
        "                for item1, item2 in product(all_w1, all_w2)])\n",
        "    wup_list.append(wup)\n",
        "\n",
        "    path = max([item1.path_similarity(item2) \\\n",
        "                for item1, item2 in product(all_w1, all_w2)])\n",
        "    path_list.append(path)\n",
        "\n",
        "    true_list.append(score_map[(w1, w2)])\n",
        "\n",
        "  except Exception as e:\n",
        "    print(w1, w2, \"error:\", e)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drink eat error: max() arg is an empty sequence\n",
            "stock live error: max() arg is an empty sequence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAjuTLB0fD-I"
      },
      "source": [
        "## Calculate Spearman's rank correlation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXnCdw8wxcVd",
        "outputId": "281dd038-e3cc-463d-abdc-c9a0ea90e8be"
      },
      "source": [
        "from scipy.stats import spearmanr\n",
        "\n",
        "coef, p = spearmanr(wup_list, true_list)\n",
        "print(\"wup  Spearman R: %.4f\" % coef)\n",
        "\n",
        "coef, p = spearmanr(path_list, true_list)\n",
        "print(\"path Spearman R: %.4f\" % coef)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wup  Spearman R: 0.6438\n",
            "path Spearman R: 0.6176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Individual task\n",
        "\n",
        "Click the link to see a sample from WordSim353 dataset. Your task is to calculate similarity measures for all elements of the corresponding word pair synset. Use the maximum similarity measure among corresponding synset elements as the similarity. Use the following similarity measures: path-based similarity (`path_similarity`), Leacock-Chodorow measure (`lch_similarity`) and Wu-Palmer measure (`wup_similarity`). Calculate Spearman's rank coefficient for every similarity measure, using a known expert estimate (`Score` column in the set)."
      ],
      "metadata": {
        "id": "oqjY0ywPi11u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Enter the coefficient of Spearman's rank correlation between the measures obtained by `path_similarity` method and known expert estimates."
      ],
      "metadata": {
        "id": "rqVR8PZLjHEs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D4b56pkojOJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GNJ5yN1-jJ4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Enter the coefficient of Spearman's rank correlation between the measures obtained by `lch_similarity` method and known expert estimates."
      ],
      "metadata": {
        "id": "pojZy2mxjKAX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rCnlJMH-i76j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vz3k1xCKjRIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Enter the coefficient of Spearman's rank correlation between the measures obtained by `wup_similarity` method and known expert estimates."
      ],
      "metadata": {
        "id": "VT1JFcR7jRPU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uVB3hg4GjT6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GNr8FDpljT9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using `hyponyms()` method find the hyponyms number for the `furnace.n.01` synset, and find the value of the first hyponym from the list using `name()` method.\n",
        "\n",
        "Enter the number of hyponyms for the `furnace.n.01` synset:"
      ],
      "metadata": {
        "id": "L0pBf4D4jUFy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gD3CkSNUjdfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1B4zsytyjdht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iwchc2aOjdor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The first element of the hyponyms list of the `furnace.n.01` synset:"
      ],
      "metadata": {
        "id": "nl6iYwxxjd7W"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cpbkjk88jqJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IP4BMDTujqMS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}