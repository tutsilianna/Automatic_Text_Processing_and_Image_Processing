{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tutsilianna/Automatic_Text_Processing_and_Image_Processing/blob/main/Vector%20Semantics/Task_5_%7C_Vector_Semantics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQp47ajD2Ga4"
      },
      "source": [
        "# Basics of word2vec\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_Pat15ls9N8"
      },
      "source": [
        "## Download the model\n",
        "Download <code>google-news-vectors</code> model. Open it using the <code>gensim</code> library."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "username = \"davydovakristina\"\n",
        "key = \"ad16991f89c9aeee22b85210aa804893\""
      ],
      "metadata": {
        "id": "e_SiAnAfTR1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nd-xNyAGy1tT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f48bea7a-8be0-47ec-b0f1-8c037de07cb6"
      },
      "source": [
        "! pip install -q -U gensim\n",
        "! pip install -q opendatasets\n",
        "import opendatasets as od\n",
        "import pandas\n",
        "\n",
        "od.download(\"https://www.kaggle.com/datasets/leadbest/googlenewsvectorsnegative300\")\n",
        "\n",
        "# To quickly load data, enter your login and token from Kaggle\n",
        "# or remove the last line, upload the file locally to colab (slow)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: davydovakristina\n",
            "Your Kaggle Key: ··········\n",
            "Dataset URL: https://www.kaggle.com/datasets/leadbest/googlenewsvectorsnegative300\n",
            "Downloading googlenewsvectorsnegative300.zip to ./googlenewsvectorsnegative300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3.17G/3.17G [02:23<00:00, 23.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4xfcycMynhZ"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "w = KeyedVectors.load_word2vec_format(\"/content/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin\",\n",
        "                                      binary=True)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6JtQjaORfzA"
      },
      "source": [
        "The structure is entitled <code>KeyedVectors</code> and in essence it is an embedding between the keys and the vectors. Each vector is identified by its search key, this is most often a short string token,  therefore, it's normally a correspondance between\n",
        "\n",
        "<center><code>{str => 1D numpy array}</code></center><br/>\n",
        "\n",
        "\n",
        "\n",
        "For example, let's dispaly first 10 coordinates of a vector, corresponding to the word <code>sunrise</code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ol9DuE6VRfzH",
        "outputId": "8f99658a-3d4f-4d7c-8937-90657d4583e9"
      },
      "source": [
        "print(gensim.__version__)\n",
        "print(\"Vector size: \", w[\"sunrise\"].shape)\n",
        "print(\"The first 10 coordinates of a vector: \\n\", w[\"sunrise\"][:10])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.3.2\n",
            "Vector size:  (300,)\n",
            "The first 10 coordinates of a vector: \n",
            " [-0.22558594 -0.03540039 -0.21679688  0.03613281 -0.2265625  -0.09814453\n",
            "  0.109375   -0.34570312  0.18652344  0.01806641]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rv9Rqvq2af8"
      },
      "source": [
        "## Task 1. Similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mns8gpJFRfzd"
      },
      "source": [
        "Build vectors for the words <code>London</code>, <code>England</code>, <code>Moscow</code>. Compute the cosine distance between the words <code>London</code> and <code>England</code> and between the words <code>Moscow</code> and <code>England</code>. In which pair the words are more similar to each other?\n",
        "\n",
        "Hint: to compute cosine distance use the <code>distance()</code> method. The correct answer is presented in the outputs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial import distance as ds\n",
        "\n",
        "print(ds.cosine(w['London'], w['England']), ds.cosine(w['Moscow'], w['England']))\n",
        "print(0.5600714385509491, 0.8476868271827698)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVRJReRO2kJY",
        "outputId": "4db0fe4f-52e1-4ffc-f070-3094876f3a88"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5600714087486267 0.8476868271827698\n",
            "0.5600714385509491 0.8476868271827698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "s9IrMkVi3Crm",
        "outputId": "48d21eeb-5d5a-47e6-a214-48855883b0d0"
      },
      "source": [
        "assert ds.cosine(w['Moscow'], w['England']) == 0.8476868271827698\n",
        "assert ds.cosine(w['London'], w['England']) == 0.5600714385509491"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-67d26cb9861d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Moscow'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'England'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.8476868271827698\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'London'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'England'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.5600714385509491\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer = \"Moscow & England\" if ds.cosine(w['Moscow'], w['England']) > ds.cosine(w['London'], w['England']) else \"London & England\"\n",
        "print(\"In which pair the words are more similar to each other?\", answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZY6kdbj_GbI",
        "outputId": "a6c7c45c-0145-4780-f17f-3b3b874f4e31"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In which pair the words are more similar to each other? Moscow & England\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLXEcSxt3DG4"
      },
      "source": [
        "## Task 2. Analogies.\n",
        "Using the most_similar method solve the analogy\n",
        "```London : England = Moscow : X```\n",
        "\n",
        "The correct answer is in the outputs.\n",
        "\n",
        "(Hint: use the following arguments: positive and negative)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4Pqub5c3DV8",
        "outputId": "01f0e9c3-ec3b-4341-c692-d7a86555de9e"
      },
      "source": [
        "result = w.most_similar(positive=['Moscow', 'England'], negative=['London'])\n",
        "\n",
        "print(f\"London : England = Moscow : {result[0][0]}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "London : England = Moscow : Russia\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRr7VikrAqBO",
        "outputId": "444fdc97-69bb-41d5-be8a-6a65cecc6469"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Russia', 0.6502718329429626),\n",
              " ('Ukraine', 0.5879061818122864),\n",
              " ('Belarus', 0.5666376352310181),\n",
              " ('Azerbaijan', 0.5418694615364075),\n",
              " ('Armenia', 0.5300518870353699),\n",
              " ('Poland', 0.5253247618675232),\n",
              " ('coach_Georgy_Yartsev', 0.5220180749893188),\n",
              " ('Russian', 0.5214669108390808),\n",
              " ('Croatia', 0.5166040658950806),\n",
              " ('Moldova', 0.5125792026519775)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[('Russia', 0.6502717733383179),\n",
        " ('Ukraine', 0.5879061818122864),\n",
        " ('Belarus', 0.5666375756263733),\n",
        " ('Azerbaijan', 0.5418694019317627),\n",
        " ('Armenia', 0.5300518870353699),\n",
        " ('Poland', 0.525324821472168),\n",
        " ('coach_Georgy_Yartsev', 0.5220180749893188),\n",
        " ('Russian', 0.5214669108390808),\n",
        " ('Croatia', 0.5166041851043701),\n",
        " ('Moldova', 0.5125792026519775)]"
      ],
      "metadata": {
        "id": "s3prZtE2yRYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert result == [('Russia', 0.6502717733383179), ('Ukraine', 0.5879061818122864), ('Belarus', 0.5666375756263733), ('Azerbaijan', 0.5418694019317627), ('Armenia', 0.5300518870353699), ('Poland', 0.525324821472168), ('coach_Georgy_Yartsev', 0.5220180749893188), ('Russian', 0.5214669108390808), ('Croatia', 0.5166041851043701), ('Moldova', 0.5125792026519775)]"
      ],
      "metadata": {
        "id": "xui_DZS-AbJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFzneqrn3Djq"
      },
      "source": [
        "## Taks 3. Similarity: find the odd-one-out word.\n",
        "Using the <code>doesnt_match</code> method, find the odd-one-out word in the string <code>breakfast cereal dinner lunch</code>.\n",
        "\n",
        "The correct answer is in the outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "493uH-D33DxJ"
      },
      "source": [
        "assert \"odd-one-out word:  \" + w.doesnt_match(\"breakfast cereal dinner lunch\".split(' ')) == \"odd-one-out word:  cereal\""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT-Zl3YaRf0X"
      },
      "source": [
        "## Task 4. Sentence vector representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dm_SiyjU3D9G"
      },
      "source": [
        "\n",
        "A sentence is given: <code>the quick brown fox jumps over the lazy dog</code>. You need to represent this sentence as a vector. Therefore, build the vector representation for each word in the model, and then average the vectors component-wise.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FbM9gOT3Ofg"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "text = \"the quick brown fox jumps over the lazy dog\".split()\n",
        "\n",
        "vectors = [w[word] for word in text]\n",
        "\n",
        "assert f\"First 5 coordinates of a sentence-vector: {np.mean(vectors, axis=0)[:5]}\" == \"First 5 coordinates of a sentence-vector: [ 0.09055582  0.05434163 -0.06713867  0.10968696 -0.01060655]\""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU3hwN53r5un"
      },
      "source": [
        "# Two models comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-HvrEkHtFqQ"
      },
      "source": [
        "## Download one more model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z13Io-4x3Ve2"
      },
      "source": [
        "\n",
        "Let's read the google-news-vectors model and the model, trained on British national corpus http://vectors.nlpl.eu/repository/20/0.zip, using gensim.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QPYDnlz3X2B",
        "outputId": "459a4cb8-2075-436c-c748-85ba6b8297fc"
      },
      "source": [
        "! wget -c http://vectors.nlpl.eu/repository/20/0.zip\n",
        "! unzip 0.zip\n",
        "! head -3 model.txt"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-05 06:06:18--  http://vectors.nlpl.eu/repository/20/0.zip\n",
            "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.181\n",
            "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.181|:80... connected.\n",
            "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n",
            "Archive:  0.zip\n",
            "replace meta.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: 163473 300\n",
            "say_VERB -0.008861 0.097097 0.100236 0.070044 -0.079279 0.000923 -0.012829 0.064301 -0.029405 -0.009858 -0.017753 0.063115 0.033623 0.019805 0.052704 -0.100458 0.089387 -0.040792 -0.088936 0.110212 -0.044749 0.077675 -0.017062 -0.063745 -0.009502 -0.079371 0.066952 -0.070209 0.063761 -0.038194 -0.046252 0.049983 -0.094985 -0.086341 0.024665 -0.112857 -0.038358 -0.007008 -0.010063 -0.000183 0.068841 0.024942 -0.042561 -0.044576 0.010776 0.006323 0.088285 -0.062522 0.028216 0.088291 0.033231 -0.033732 -0.002995 0.118994 0.000453 0.158588 -0.044475 -0.137629 0.066080 0.062824 -0.128369 -0.087959 0.028080 0.070063 0.046700 -0.083278 -0.118428 0.071118 0.100757 0.017944 0.026296 0.017282 -0.082127 -0.006148 0.002967 -0.032857 -0.076493 -0.072842 -0.055179 -0.081703 0.011437 -0.038698 -0.062540 -0.027899 0.087635 0.031870 0.029164 0.000524 -0.039895 -0.055559 0.024582 -0.030595 0.003942 -0.034500 0.003012 -0.023863 0.033831 0.061476 -0.090183 -0.039206 -0.026586 -0.042763 0.049835 -0.052496 -0.020044 0.073703 0.096775 0.033063 0.000313 -0.022581 -0.141154 0.032095 0.077733 -0.063739 -0.055647 -0.017604 0.044639 -0.062925 -0.001960 0.024665 -0.009416 -0.021381 0.082724 -0.031026 0.027255 0.066198 0.000845 0.008393 0.039434 0.054104 -0.060255 0.034266 0.079435 0.043624 -0.015871 -0.038030 -0.030374 -0.020542 0.007132 0.008708 0.087840 0.017351 -0.089493 0.030182 0.026961 -0.071212 -0.004854 0.007389 0.067203 -0.026351 -0.011460 -0.058723 0.013153 -0.020313 -0.051170 0.002242 0.088222 -0.004267 -0.073523 -0.021874 -0.033585 -0.048553 -0.019119 -0.025310 0.053096 0.111063 0.035042 -0.082811 -0.073749 -0.010048 0.012265 -0.023893 -0.125340 0.026611 0.043258 -0.010473 -0.044428 -0.039251 -0.046891 -0.013008 0.062219 0.078732 -0.086303 0.016901 0.010331 -0.043754 -0.057733 -0.037964 0.024907 0.068143 -0.019992 -0.035030 0.038854 0.034345 -0.048839 -0.105419 0.043013 -0.023374 -0.077629 -0.076465 0.078564 -0.024519 0.041293 -0.032088 -0.007053 0.022618 -0.004657 -0.093970 -0.000199 0.004813 -0.044789 -0.127900 -0.033516 -0.043816 0.033056 -0.057619 0.004901 0.018863 0.039752 0.000739 -0.136350 -0.067819 -0.014856 0.058351 -0.014275 -0.000873 -0.039388 -0.017191 -0.051184 -0.046863 0.006143 -0.075998 -0.064695 0.046676 -0.020558 0.082474 0.160449 -0.027475 0.009541 -0.021876 0.027416 0.078049 0.089309 0.032928 -0.033272 0.048905 0.061164 0.054811 0.024527 -0.034978 -0.018083 -0.077601 0.034112 -0.021121 0.098856 0.019585 -0.058928 -0.016126 -0.011748 0.031588 0.003205 -0.077483 -0.002372 -0.113548 0.047445 -0.027094 -0.032843 0.042378 -0.074703 0.057001 0.012020 0.131156 0.002080 -0.065770 0.112443 0.047786 0.024492 -0.108401 0.016836 0.001478 0.041542 -0.067801 0.102876 -0.052808 -0.136035 0.073852 0.079966 -0.000586 0.034055 -0.053040 0.050461 -0.021550 0.014827 0.077605 -0.024783 -0.082388 0.074410 -0.033689 -0.010982 0.043733\n",
            "go_VERB 0.010490 0.094733 0.143699 0.040344 -0.103710 -0.000016 -0.014351 0.019653 0.069472 -0.046938 -0.057882 0.076405 -0.025230 0.026663 0.029986 -0.001605 -0.027803 0.037521 -0.050608 0.016215 0.025947 0.061172 -0.037448 -0.079232 0.071731 -0.085143 0.021494 -0.135554 -0.026115 -0.066408 0.022858 0.083231 0.020998 -0.049906 -0.079992 -0.060827 -0.028916 -0.029005 0.026067 -0.074869 0.073802 0.023593 -0.024348 -0.093236 0.006169 0.013119 0.007817 -0.088096 -0.012373 0.099807 0.011438 0.028583 0.025614 0.175403 0.007033 0.038856 0.004040 -0.088907 0.079697 0.037448 -0.128230 -0.066502 -0.018969 0.025777 0.035905 0.003710 -0.089079 0.071521 0.039237 0.052136 0.020986 -0.030793 -0.069486 -0.137115 0.008305 0.020813 -0.155342 0.000619 -0.033499 -0.104162 -0.061528 -0.043877 -0.042524 -0.032872 0.045071 0.072908 0.096057 0.141987 -0.078056 -0.013102 -0.026589 -0.073783 0.114807 0.077389 -0.041879 -0.052886 0.053710 0.036806 -0.035973 0.049071 -0.107199 -0.043581 0.016515 -0.029278 -0.026228 0.068037 -0.024183 0.040984 -0.020469 -0.103833 -0.007225 -0.073788 -0.051063 -0.037850 0.052581 -0.053090 -0.012198 -0.057343 0.024050 -0.046498 0.003065 -0.058912 0.043695 0.006340 0.060953 -0.008608 -0.029686 0.081187 -0.020058 0.059240 -0.061306 -0.002190 -0.020671 0.076712 0.049087 0.001153 0.087481 0.008559 0.069936 -0.015886 0.006122 0.038000 -0.071984 0.005263 0.060463 -0.051217 -0.034060 0.045217 0.059163 -0.048462 -0.005371 0.009663 0.081303 0.051019 -0.001248 -0.022637 0.016228 -0.006395 -0.053985 -0.014513 -0.017219 -0.010658 -0.012446 -0.035279 -0.003882 0.036453 0.029681 0.021278 0.006188 0.027861 0.076864 -0.042835 -0.022834 0.013928 0.066150 0.040982 -0.110985 -0.018865 0.006675 0.019173 0.021484 -0.021977 -0.035462 0.000464 -0.024281 0.010881 -0.064037 -0.024893 -0.095968 0.020834 -0.114225 -0.023433 -0.043971 0.014273 0.013481 -0.007542 0.079197 0.021280 -0.129871 0.080770 0.028912 -0.044134 -0.019904 -0.039406 -0.076024 0.058488 -0.094331 -0.082633 0.017676 -0.084006 -0.024444 -0.049778 -0.044615 -0.013499 -0.036736 -0.038579 -0.117319 0.012026 -0.007846 0.024003 -0.101645 0.111720 -0.010241 0.050279 -0.002212 0.060056 -0.116837 0.006078 -0.017954 -0.021794 0.020252 -0.031337 -0.032407 0.081086 -0.095125 0.041699 0.015953 -0.045653 -0.022522 -0.021422 -0.029167 0.052594 0.016523 0.081598 -0.027877 0.000609 0.012837 0.011880 0.074220 0.009736 0.006465 -0.140252 0.010762 -0.038319 0.038924 0.042537 0.005027 0.014024 0.024548 0.050131 -0.048069 -0.012616 -0.052162 -0.100378 0.067741 -0.067824 -0.020692 -0.043022 -0.038036 -0.016860 0.027835 0.140990 -0.045201 -0.069347 0.174518 -0.000236 0.008150 -0.039823 0.041197 0.056322 0.085883 0.027376 0.036537 0.094723 -0.103076 0.105746 0.059074 0.010947 0.099756 -0.027213 0.128793 -0.054593 0.025890 0.053512 0.005200 -0.035256 0.063273 -0.027069 0.046354 -0.002262\n",
            "make_VERB -0.013029 0.038892 0.008581 0.056925 -0.100181 0.011566 -0.072478 0.156239 0.038442 -0.073817 -0.000439 0.114153 -0.051814 -0.056424 -0.038872 0.054174 0.000059 0.039477 -0.021345 0.053860 -0.131669 -0.020844 0.012362 -0.016145 0.048171 -0.122080 0.028292 -0.043984 -0.025178 -0.006927 -0.029133 -0.085539 -0.086455 0.001830 -0.099361 -0.029536 0.071144 -0.003143 0.027941 -0.035858 0.026530 0.004768 0.021307 -0.065139 -0.053572 0.038951 0.045786 -0.045258 -0.037586 0.038983 -0.062755 -0.000504 0.044502 0.123845 -0.050279 0.030425 -0.067798 -0.037958 0.023805 -0.011021 -0.041084 -0.090643 0.130500 0.046460 -0.040764 0.020988 -0.087054 -0.017896 0.056193 0.007352 -0.019590 -0.048728 -0.027895 -0.027241 -0.038715 0.008038 -0.172688 -0.106911 -0.012085 -0.050829 -0.053590 -0.059879 -0.030488 -0.025220 0.020381 0.102120 0.041989 0.119341 -0.006702 0.035009 0.016077 -0.014298 0.124971 0.050049 0.113425 -0.027587 -0.001379 -0.031188 0.041054 -0.013872 -0.134232 -0.073757 0.075578 -0.064260 0.035823 0.032695 -0.059019 0.086900 -0.049042 -0.105385 -0.024058 0.095202 -0.044429 -0.053781 -0.013759 -0.077265 -0.043720 -0.082217 0.128089 -0.041757 -0.023743 0.027764 0.008487 -0.022274 -0.023357 -0.013653 0.047372 0.098364 -0.020791 -0.063818 0.055996 -0.007599 0.018954 -0.003601 0.055991 -0.089158 0.008229 -0.027915 0.056351 0.101133 0.043454 0.026218 0.010540 0.053571 0.079725 -0.048278 -0.048708 -0.075923 -0.045807 0.083970 -0.087983 0.058780 0.025992 -0.008407 -0.059681 -0.022862 0.099799 0.083928 -0.024096 0.008313 -0.065932 -0.003852 0.051210 -0.104068 -0.029864 0.021315 -0.036515 -0.050546 0.003077 0.007452 -0.020468 0.035296 -0.025792 -0.045913 0.042664 -0.025302 -0.057182 -0.026525 -0.053029 -0.009697 0.031003 0.064251 -0.096399 -0.020674 0.006306 -0.004981 -0.118857 -0.058013 -0.018890 0.042343 -0.111604 -0.071149 0.042898 0.094869 -0.029797 -0.134403 -0.030753 0.050269 -0.096115 0.019021 0.014348 -0.049818 -0.017920 0.044926 0.038627 -0.091947 -0.001567 0.064930 -0.065977 -0.015673 0.034979 0.064560 0.036580 -0.000075 -0.064665 -0.054986 -0.090783 -0.033908 0.106271 0.058234 -0.100301 0.015398 -0.072886 0.019940 0.066563 0.063845 -0.036548 -0.018204 -0.008618 0.098109 -0.128401 -0.053501 -0.032671 0.027777 -0.043889 -0.018033 0.099028 -0.026501 -0.026575 -0.106259 0.036872 0.024990 0.003347 0.045086 -0.083903 0.021039 0.056445 -0.053898 0.011539 -0.033661 0.020421 -0.051413 0.021900 0.075706 0.089103 -0.022953 -0.032130 -0.049067 0.014476 -0.036070 0.010638 -0.049193 -0.005560 -0.094642 -0.045530 -0.010048 0.074026 0.053386 -0.006803 0.043264 -0.004896 0.020676 0.002030 0.019262 0.043679 -0.006854 -0.064545 -0.059780 -0.070871 0.004817 0.058769 -0.052450 -0.023481 -0.036496 -0.029701 -0.002672 -0.029965 0.053667 0.038260 -0.026692 0.068764 -0.070122 0.060288 0.124118 -0.064670 -0.044363 0.023818 -0.022746 -0.086708 0.016196\n",
            "get_VERB 0.019242 0.144838 0.155635 0.009607 -0.169437 -0.004972 -0.021559 0.009400 0.074920 -0.033244 -0.032937 0.112560 0.041283 -0.030355 -0.048271 -0.061402 0.048208 0.083419 -0.043215 0.069025 -0.027292 0.097641 -0.070595 -0.034194 0.091538 -0.068585 0.012530 -0.120053 -0.014222 0.002379 0.070677 0.015263 0.030467 -0.001756 -0.013990 -0.026711 0.036041 0.014917 0.031644 -0.055844 0.115340 -0.003877 -0.045724 -0.025892 -0.024716 0.020095 -0.024788 0.005623 0.055026 0.078559 0.011337 0.033604 0.051766 0.135975 0.061593 0.029842 -0.021268 -0.136608 0.079957 0.011375 -0.155011 -0.151481 0.076298 0.031386 0.020274 0.028823 -0.127225 0.016972 0.000477 0.005670 -0.015052 -0.032207 -0.054631 -0.125453 0.027146 0.026122 -0.025028 -0.046540 0.021146 -0.082786 -0.051401 -0.015178 -0.017908 0.031622 0.061065 0.053762 0.093668 0.120508 0.013383 -0.019997 0.047220 -0.039273 0.070729 0.006117 0.033016 -0.059557 0.080700 0.044238 -0.013197 0.072017 -0.049897 -0.035379 -0.028077 -0.068918 0.048224 0.093904 -0.011614 0.143074 -0.066022 -0.104345 -0.080770 -0.034034 0.019643 -0.048965 0.020702 -0.061310 -0.077258 -0.091273 0.063781 -0.111229 0.058853 -0.006970 0.029294 0.001331 0.057966 -0.035211 0.033447 0.015224 0.027628 0.020672 0.023872 0.007849 -0.001215 0.045739 -0.032844 -0.027810 0.047459 0.011252 0.034142 -0.020341 0.063802 -0.005105 -0.014244 0.015935 -0.008431 -0.028795 -0.043172 -0.003883 0.023328 -0.022711 0.001897 0.032548 0.064574 0.097152 0.003275 -0.109298 0.024141 0.090362 -0.038664 -0.023928 -0.006557 0.025754 -0.011571 -0.053434 0.040903 0.061501 0.144468 0.017326 -0.032445 -0.024866 -0.000533 -0.067980 -0.099827 0.011754 0.026172 0.041204 -0.059723 0.026298 0.006623 -0.030971 0.030617 -0.008276 -0.084025 -0.030169 0.044463 0.002809 -0.030988 -0.025040 -0.059924 0.042590 -0.041501 0.018392 -0.107077 0.040232 -0.015956 -0.034321 0.064614 0.023561 -0.118772 0.011774 0.053385 -0.059752 -0.007313 -0.024684 0.012329 0.024288 -0.019210 -0.043125 0.031520 -0.072438 -0.043097 -0.061750 -0.049240 -0.039941 -0.086947 -0.019136 -0.082013 -0.095680 -0.012216 0.044958 -0.083804 -0.020841 -0.024199 0.085375 -0.000988 0.002353 -0.075649 -0.016678 -0.042220 0.002328 0.046584 -0.053008 -0.002773 0.059518 -0.113334 0.082102 0.038316 -0.023807 0.014160 -0.024084 0.049738 0.003309 0.020473 0.056583 -0.040877 -0.036386 0.033831 0.014504 0.005588 -0.098602 0.013935 -0.115838 0.048181 -0.013819 0.030253 -0.035629 0.022863 -0.019994 -0.016116 -0.052284 -0.034443 0.024592 -0.028994 -0.093012 -0.007058 0.013011 0.000991 -0.014438 -0.044545 0.040938 -0.043503 0.103244 -0.090978 0.005134 0.156962 0.013555 0.053622 -0.059002 0.032924 0.010204 -0.054882 -0.070490 0.102098 0.071841 -0.041202 0.079912 0.031834 0.048141 0.104557 -0.010763 0.057594 -0.091408 0.010093 0.010459 0.074382 0.028358 0.016023 -0.091680 0.031687 -0.108943\n",
            "one_NUM 0.056419 -0.021141 0.090616 -0.032564 -0.054807 0.031836 -0.004311 0.064928 -0.033537 0.008632 0.052463 0.034665 0.016636 -0.006993 0.002519 -0.049758 0.003396 0.076139 -0.040574 -0.034938 -0.018776 0.016026 -0.049461 -0.058605 0.052319 -0.022392 0.037263 -0.101570 -0.015736 0.014700 0.005736 -0.098316 0.023485 0.079696 0.000523 -0.072989 0.112214 -0.042671 -0.039667 -0.048063 0.076580 -0.044722 -0.009309 -0.029152 -0.032665 -0.017058 0.066964 -0.003375 0.010473 0.053499 -0.057250 0.022080 0.031383 0.110925 -0.070559 0.090881 0.004533 -0.072508 0.064684 0.078511 -0.096904 -0.011367 0.023967 0.045979 0.047333 -0.058862 -0.043411 0.133778 -0.040768 -0.027242 -0.013070 -0.021536 0.068636 -0.093734 -0.071668 0.021352 0.002289 0.037268 -0.065373 -0.020971 -0.023998 -0.035441 0.033266 -0.028960 0.040619 0.064281 0.065757 0.123596 0.082568 -0.004874 0.028938 0.047771 0.025916 0.021118 -0.023998 0.009172 0.100679 -0.078543 -0.046099 0.037856 -0.161992 -0.039157 0.007258 -0.039263 0.063678 0.060994 0.025443 0.036222 -0.007018 -0.108600 0.022631 -0.031306 -0.050116 0.006508 -0.018496 -0.098152 -0.043395 -0.012609 0.047591 -0.006303 0.029529 0.003471 0.004601 -0.050615 0.014272 0.049927 0.105728 0.021712 -0.030996 0.026857 -0.060846 -0.107893 0.017269 0.011160 0.020126 -0.115469 -0.015772 -0.007356 0.083882 0.119524 0.075473 -0.050709 0.036562 -0.059548 -0.053619 -0.049486 -0.124977 0.004016 -0.001549 0.007669 -0.039740 0.002400 -0.069183 -0.065801 -0.088064 -0.107159 0.072443 0.083424 -0.030178 -0.033223 0.084401 0.044171 0.013811 -0.084562 0.003194 0.056247 -0.022866 0.023806 0.047635 0.025468 0.069964 -0.096787 -0.025001 -0.021526 0.061188 0.045733 -0.099350 0.016400 -0.084030 0.056672 0.099467 -0.052893 -0.100382 0.124380 0.018206 0.034541 -0.014580 -0.124287 -0.103852 0.053850 -0.017001 -0.062173 -0.063301 0.024652 -0.023819 -0.057063 -0.013738 -0.027791 -0.013030 0.030126 -0.000573 -0.026613 -0.005942 0.057634 -0.079672 0.022556 0.000011 -0.037800 0.074457 -0.071388 0.086022 0.013307 -0.035879 -0.072396 0.034418 0.004146 -0.011763 0.010722 -0.015462 0.070248 -0.097890 -0.033176 -0.078940 0.169393 0.050572 0.045639 0.004937 0.008214 -0.024237 -0.039713 -0.082674 -0.056171 -0.114638 -0.037084 -0.019666 0.023100 0.091380 -0.070856 0.019748 0.032893 0.054247 0.001259 0.166033 -0.021817 -0.076404 -0.046031 0.002690 -0.015014 0.087486 -0.147485 0.035263 -0.095335 -0.035910 -0.060311 0.068841 0.034646 -0.004976 -0.025316 -0.066141 0.042181 -0.001685 -0.061911 -0.060588 -0.036907 -0.003193 -0.042462 -0.023907 0.015987 -0.030946 0.041203 0.003092 0.075626 0.082451 -0.077289 0.102090 -0.026878 0.057229 -0.067434 -0.041582 0.036640 0.022304 0.018005 -0.005375 0.038557 -0.087250 0.042553 -0.052500 0.070422 0.079278 -0.017086 0.055328 0.025901 0.111785 0.057416 0.022670 0.048453 -0.034165 -0.034901 0.014241 0.061734\n",
            "see_VERB 0.033784 -0.033085 0.020113 -0.010017 -0.081187 -0.021287 -0.009972 0.037831 -0.001592 0.079325 0.025121 -0.012038 -0.060439 0.023697 -0.011899 -0.009593 0.043340 -0.054792 -0.058171 0.111542 -0.072478 0.046219 -0.036414 -0.038994 0.010711 -0.022551 0.051247 -0.030018 -0.015760 -0.022421 0.029474 0.033584 0.077213 0.026976 -0.026057 -0.024171 -0.060402 -0.022088 0.036667 0.050913 0.044288 0.050425 0.020312 -0.079651 0.039995 0.025213 -0.093921 0.123022 -0.018795 -0.019389 0.002286 0.001979 -0.101728 0.107971 -0.068966 0.035741 0.015628 -0.035574 0.153279 0.094290 -0.104906 -0.052320 0.123525 0.008491 -0.095255 -0.007215 0.046057 0.073950 0.000288 0.048581 0.015934 0.018141 -0.025546 -0.197751 -0.042662 -0.052096 -0.026476 -0.062207 0.041462 -0.126658 -0.054443 0.032953 -0.042276 -0.002271 0.064819 -0.000886 0.057284 0.151726 -0.005298 0.027951 -0.030457 -0.006905 -0.063591 0.038784 0.042528 0.009519 -0.006915 0.012526 -0.026987 -0.031367 -0.032849 -0.069462 0.078398 -0.077442 0.025363 0.015721 0.006499 0.050169 0.013559 -0.041564 -0.021559 0.038734 -0.048881 -0.014219 0.018850 -0.076032 -0.031383 -0.053150 0.073602 -0.060924 0.059121 -0.135463 -0.065702 0.017648 0.056463 -0.019903 0.036354 0.063048 0.043614 -0.058998 0.036254 -0.089223 -0.017289 -0.016416 0.103582 -0.060645 -0.003059 -0.069493 0.018295 -0.066589 0.049994 0.053174 -0.001443 -0.069640 0.044240 -0.050163 -0.073309 0.004212 0.096908 0.120702 -0.067013 -0.009905 -0.043493 -0.076316 0.014215 -0.031754 -0.021288 -0.004769 -0.137067 -0.024747 -0.031678 0.015084 0.031505 -0.103574 -0.066272 0.042509 0.060396 -0.059968 0.002428 -0.024155 0.063192 -0.077420 -0.094139 0.057205 0.022803 0.069838 -0.001844 0.020190 -0.049075 -0.048242 0.025786 0.067670 -0.152904 0.038227 0.090410 -0.012939 -0.040886 -0.011291 0.012498 -0.020443 -0.048081 0.018145 0.055328 0.065196 -0.062635 -0.106917 0.064730 0.044867 -0.068668 -0.037746 0.057211 -0.034426 0.051337 -0.025572 0.046295 -0.011716 -0.006654 -0.030194 -0.051332 0.026101 0.104883 -0.007145 -0.070217 -0.000867 0.002503 -0.064973 0.037842 0.021236 0.023925 -0.017935 -0.079966 0.055120 -0.034364 0.071393 -0.029756 0.055380 -0.002203 0.004057 0.018498 0.023873 -0.042996 -0.027251 -0.031769 -0.023764 -0.056463 -0.019280 0.102642 -0.095517 0.071215 0.080328 -0.003316 0.068579 0.052600 0.053909 0.108883 0.012581 0.043106 0.031879 0.054328 -0.030787 -0.028546 -0.068817 -0.013480 0.036747 0.036505 -0.026683 0.034958 0.075650 0.001530 0.077361 -0.012978 -0.059580 -0.003257 -0.075410 0.026864 -0.090516 0.148559 -0.067887 -0.009229 0.107859 0.011224 0.026525 -0.020502 0.029137 0.059563 0.094475 0.072240 -0.095192 0.006081 0.021361 0.136805 -0.011188 -0.004593 0.007071 -0.166692 0.081480 0.042853 -0.013095 0.103005 0.036355 -0.033840 -0.017550 0.021768 0.056411 -0.041009 -0.057657 0.068774 0.006003 0.083118 -0.005328\n",
            "time_NOUN 0.028551 -0.088598 0.050082 0.007181 -0.104172 0.101837 0.035492 -0.016076 -0.036470 -0.031597 0.008517 0.081070 0.005958 0.011227 0.055194 -0.085483 -0.028711 -0.041737 -0.087045 0.040034 -0.014075 0.115195 0.014005 -0.067294 0.107071 -0.100702 -0.009378 -0.124226 0.017455 0.003322 0.005558 -0.061389 0.162625 -0.079217 -0.014027 0.008225 0.009872 0.019838 0.037906 -0.081853 0.025483 0.006533 -0.027086 -0.061254 0.021571 -0.058520 -0.030755 -0.037765 0.013022 0.099744 -0.015871 -0.040148 0.013853 0.174780 -0.030214 0.036497 -0.030496 -0.072386 0.129520 -0.039360 -0.012611 -0.044570 0.009870 0.027195 -0.018254 -0.064432 -0.027389 0.076080 0.017096 0.094141 -0.004854 -0.119384 -0.079426 -0.096194 -0.061261 -0.046198 -0.066695 -0.095495 0.071205 -0.035128 -0.109664 0.000560 -0.008151 0.001801 0.033679 0.077297 0.069809 0.126302 -0.106770 0.054766 -0.059580 -0.031521 0.100238 0.050529 -0.093611 0.014521 0.113995 0.047877 0.043517 0.046725 -0.064180 -0.012568 -0.001566 -0.058337 0.033448 0.017877 -0.060126 0.057385 -0.074544 0.033809 -0.063151 0.051488 -0.082470 0.012745 -0.012867 -0.022761 -0.073556 -0.064041 0.072679 0.004931 0.033317 -0.036562 0.097679 0.064440 0.006349 0.029687 0.079091 0.126180 0.017304 -0.055640 -0.004360 -0.071465 0.020999 0.020292 0.020851 -0.059998 0.026812 -0.025640 0.056559 0.062663 0.068633 0.003051 0.038869 -0.033266 -0.003146 -0.004676 -0.060268 -0.099329 -0.011529 -0.024929 0.052014 0.016963 0.007527 -0.074523 -0.107231 -0.095747 0.040862 0.055063 0.037597 0.030260 -0.060898 -0.035649 0.030211 -0.061037 -0.030174 0.025636 0.006325 -0.005351 -0.055750 0.042350 0.005378 -0.067135 -0.089138 -0.024828 -0.011560 0.063750 -0.050910 -0.029080 -0.003223 0.070240 -0.015769 0.085594 -0.091409 0.066143 -0.005699 -0.074672 0.015766 -0.008242 -0.016767 0.064410 -0.070908 -0.074450 0.005181 0.062501 -0.007251 -0.036160 -0.030252 -0.006774 -0.117676 0.085261 0.055248 0.023099 -0.001916 0.010082 0.004036 0.009452 0.009258 0.015016 0.040363 0.048659 0.021832 -0.100119 -0.093323 0.046102 0.003035 -0.010301 0.019116 0.004175 0.018846 0.026686 -0.200220 0.021341 -0.039848 0.050640 -0.026792 0.057337 -0.060018 0.013275 -0.043792 -0.020855 0.018315 -0.052217 -0.129547 -0.029040 -0.041891 -0.064859 -0.007379 -0.078831 -0.039299 -0.070625 -0.035255 0.058762 0.083695 0.085014 -0.052283 -0.005346 0.002431 0.030622 0.076632 -0.101137 0.037876 -0.031314 0.000538 -0.022329 0.090587 0.060532 -0.022718 -0.007348 0.040430 0.084318 -0.037918 -0.041699 0.064522 -0.004134 -0.056841 0.021044 0.019699 -0.000656 -0.051270 -0.011215 -0.010951 -0.015126 -0.095848 -0.043651 0.068587 -0.033160 0.112103 -0.069295 0.069698 -0.027304 -0.032045 0.011533 0.078525 0.012872 -0.072700 -0.018674 -0.048350 0.033240 0.006589 0.038484 0.049222 -0.036532 0.050987 0.054671 -0.073558 -0.075419 -0.051517 -0.082573 -0.022406 0.042652\n",
            "take_VERB -0.085915 0.072050 0.061177 0.034385 -0.130897 -0.020019 0.041692 -0.001646 -0.000336 -0.085338 0.038205 0.018875 -0.083286 -0.061232 0.016179 0.066189 0.060670 0.000543 -0.135004 0.049699 -0.012316 -0.007948 -0.024639 -0.000349 -0.021084 -0.054148 0.061099 -0.111185 0.005860 -0.023507 0.029399 0.055637 0.049720 0.019389 0.024483 0.000545 -0.015728 -0.084071 -0.045200 -0.039850 -0.057122 -0.043768 -0.087916 -0.003074 0.094422 0.002184 0.054202 -0.081253 0.060053 0.047244 0.096669 0.020689 0.049919 0.160756 0.074185 0.068677 0.081737 0.046915 0.130613 0.003062 -0.094222 -0.038692 0.007875 0.048645 0.055408 -0.037991 -0.064129 0.057689 0.079305 0.059282 0.004156 0.006521 0.049386 -0.046141 -0.058870 -0.012268 -0.029792 -0.073071 -0.006719 -0.020783 -0.053037 -0.029306 -0.123163 -0.016259 -0.005775 -0.005656 0.082892 0.182866 -0.053775 0.081097 0.031257 -0.082803 0.072860 0.058470 -0.042368 -0.044028 -0.007983 0.027884 0.029243 -0.005043 -0.027391 -0.081547 0.087199 -0.054481 0.092473 0.073842 0.080296 0.116464 -0.060440 -0.112892 0.000785 0.031157 -0.055366 -0.042485 -0.029086 -0.058525 0.026030 -0.063302 0.052599 -0.005851 0.048085 -0.056849 -0.002640 0.003858 -0.009217 0.042292 0.014251 0.111839 0.003789 0.004826 -0.055015 0.037018 0.051596 0.034754 0.054819 -0.125969 -0.021572 -0.046395 0.050502 0.015387 0.046864 0.004564 0.036650 0.010489 0.024583 -0.036233 -0.043142 -0.063384 -0.008670 0.051395 0.047257 -0.015780 0.067077 -0.027996 -0.014117 -0.058627 0.018429 0.073644 0.050070 0.010536 0.027303 -0.002755 0.026361 -0.045678 -0.013685 0.031982 -0.006479 0.034540 -0.003912 0.057480 -0.007845 -0.097392 0.028641 -0.050393 0.085079 0.011244 -0.097678 0.007091 -0.027963 0.022363 0.002283 0.037801 -0.037196 0.169746 -0.038624 0.078767 -0.001875 0.054219 -0.052247 0.026505 -0.138177 -0.021248 -0.073399 0.058894 -0.001102 -0.008011 -0.044078 -0.001408 -0.078455 0.096208 -0.010742 -0.043702 0.079016 -0.018590 0.013306 0.022266 0.032821 0.004880 -0.002337 0.045593 -0.024818 -0.007891 0.008684 -0.061167 -0.045740 -0.051571 -0.049655 -0.008531 -0.006200 0.048838 -0.072153 0.049921 -0.059215 0.127064 0.040236 0.098747 -0.042659 -0.028465 -0.056385 0.065828 0.036884 -0.096360 0.006464 0.006201 0.056877 0.071954 0.114779 -0.087398 0.008548 0.044174 0.007840 -0.017414 0.046436 0.096034 -0.127091 -0.028683 -0.013463 -0.020997 -0.010382 -0.030867 0.037462 -0.108436 0.018027 -0.034779 0.058547 -0.068791 0.021402 -0.011831 0.002013 0.060412 -0.078119 -0.019209 0.013182 -0.094492 0.110161 -0.063736 0.046850 0.022188 0.007599 -0.007369 -0.058936 0.056017 -0.049103 -0.057295 0.058677 0.147318 0.107422 -0.079093 -0.010623 0.073036 0.024813 -0.045329 0.095850 0.046119 -0.086194 0.039533 -0.001355 0.052867 0.066449 -0.062630 0.020811 -0.032284 -0.087716 0.034099 -0.028051 -0.021583 -0.030066 -0.009957 0.022330 0.032838\n",
            "know_VERB -0.025984 0.067260 0.034687 0.040777 -0.161350 0.013020 -0.007056 -0.024733 0.058577 0.032559 -0.012518 0.108795 0.025453 -0.025084 -0.102243 -0.040177 -0.021312 -0.043816 -0.081668 0.091292 0.038419 0.055116 0.035742 -0.050125 -0.018741 -0.049817 0.012086 -0.079459 -0.000711 -0.023625 0.013238 0.074507 0.030027 -0.001057 0.035990 -0.050576 -0.003847 0.060832 0.011826 0.008473 -0.015987 -0.049248 -0.043038 -0.075997 0.000403 0.001103 -0.003824 -0.029661 0.098314 0.062112 -0.109037 -0.013936 -0.008971 0.112062 -0.019038 -0.048354 0.006206 -0.070438 0.065072 0.018215 -0.104006 -0.086250 0.071208 0.083047 -0.001287 -0.120081 -0.014862 0.129383 0.042387 -0.065375 0.006762 -0.073221 -0.010304 -0.115515 0.007240 0.022477 -0.058240 -0.010925 -0.000522 -0.142643 0.035225 0.062995 -0.036440 -0.023326 0.043814 0.082391 0.105983 0.029809 -0.019996 0.034061 0.027599 -0.003354 -0.002728 0.053269 -0.012316 -0.085819 0.030923 0.038884 -0.022784 0.034281 -0.092045 -0.079878 -0.006614 -0.093812 0.035342 0.094636 -0.057155 0.051942 0.024737 -0.080949 -0.065721 -0.039007 -0.005870 -0.073761 0.076176 -0.017809 0.005946 -0.080602 0.032073 -0.102764 0.038929 -0.075056 -0.013560 -0.018190 0.069482 0.083503 0.027585 0.023521 0.014033 -0.036928 -0.005933 0.030368 0.006602 0.030084 0.044758 -0.018216 0.035520 -0.012898 0.045371 -0.001641 0.110703 0.017023 0.005551 0.045749 -0.005427 -0.074813 -0.010974 -0.010148 0.095004 0.013014 -0.005125 0.016952 -0.030893 -0.062859 -0.080818 -0.051509 0.006347 0.010265 -0.063437 0.009479 0.009224 0.006066 -0.008744 -0.104529 -0.025839 0.070249 0.095893 -0.047559 0.013237 0.055287 0.009931 -0.023470 -0.098192 -0.030587 0.049573 -0.017828 -0.071414 -0.018803 -0.014656 -0.002473 0.036026 0.041413 -0.054189 0.018682 0.084050 -0.029306 -0.026105 -0.016993 -0.026153 0.102438 -0.018404 -0.094386 0.000044 0.060043 -0.082400 -0.121982 -0.031880 -0.045073 -0.080735 0.012308 0.069106 -0.024642 0.040432 0.055083 -0.017416 0.011888 -0.057825 -0.112030 -0.016259 0.037437 0.081257 -0.035797 -0.070929 -0.093251 -0.083276 -0.047928 -0.043073 -0.057629 0.043480 0.033591 -0.152529 0.020240 0.020165 0.082562 -0.033501 0.070734 -0.085496 0.029878 -0.066357 0.026343 -0.050701 -0.037989 -0.134138 0.017599 -0.078392 0.056217 -0.005646 0.014984 0.065454 -0.014985 -0.032853 0.037999 0.024869 0.056706 0.094115 0.001015 0.021370 0.050369 0.088182 -0.045817 -0.098003 -0.092237 0.017012 -0.017444 0.094234 -0.003846 -0.046259 -0.039507 0.024746 0.046465 -0.040855 0.020798 -0.003858 -0.007113 0.013065 -0.084681 0.009278 0.061606 -0.135605 -0.049061 0.016939 0.093177 0.044400 -0.003246 0.153727 0.036902 0.033454 -0.019347 -0.016304 -0.004672 0.098153 0.026940 0.091487 0.098230 -0.060728 0.079308 -0.048837 0.049123 0.109093 -0.037728 0.096776 -0.065414 0.025478 0.105321 -0.037531 -0.086120 0.071593 -0.032246 -0.014722 -0.013469\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-21wScRRf1E"
      },
      "source": [
        "Let's download the model, trained on the British national corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-E6OAvhw8-A7"
      },
      "source": [
        "w_british = KeyedVectors.load_word2vec_format(\"model.bin\", binary=True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GpisLDDRf1T"
      },
      "source": [
        "Note, that the vector size also equals 300 in this case. Specify the part of speech of the word of interest by means of underscore . All words should be lowercased."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7VEcvPIRf1W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9309243-7a18-4514-c010-d1f2b0eeab3a"
      },
      "source": [
        "try:\n",
        "    print(w_british[\"London_NOUN\"].shape)\n",
        "    print('upper is ok')\n",
        "except:\n",
        "    print(w_british[\"london_NOUN\"].shape)\n",
        "    print('lower is ok')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(300,)\n",
            "lower is ok\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfpohw153YQs"
      },
      "source": [
        "## The dataset for the quality evaluation\n",
        "Let's download the wordsim353 dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6c2--gQ3bJF",
        "outputId": "09c375fd-fc7c-498a-d942-ff6774d565cb"
      },
      "source": [
        "! wget -c http://alfonseca.org/pubs/ws353simrel.tar.gz\n",
        "! tar -xvf ws353simrel.tar.gz\n",
        "! head -5 wordsim353_sim_rel/wordsim_similarity_goldstandard.txt"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-05 06:01:13--  http://alfonseca.org/pubs/ws353simrel.tar.gz\n",
            "Resolving alfonseca.org (alfonseca.org)... 162.215.249.67\n",
            "Connecting to alfonseca.org (alfonseca.org)|162.215.249.67|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5460 (5.3K) [application/x-gzip]\n",
            "Saving to: ‘ws353simrel.tar.gz’\n",
            "\n",
            "ws353simrel.tar.gz  100%[===================>]   5.33K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-05-05 06:01:13 (211 MB/s) - ‘ws353simrel.tar.gz’ saved [5460/5460]\n",
            "\n",
            "wordsim353_sim_rel/wordsim353_agreed.txt\n",
            "wordsim353_sim_rel/wordsim353_annotator1.txt\n",
            "wordsim353_sim_rel/wordsim353_annotator2.txt\n",
            "wordsim353_sim_rel/wordsim_relatedness_goldstandard.txt\n",
            "wordsim353_sim_rel/wordsim_similarity_goldstandard.txt\n",
            "tiger\tcat\t7.35\n",
            "tiger\ttiger\t10.00\n",
            "plane\tcar\t5.77\n",
            "train\tcar\t6.31\n",
            "television\tradio\t6.77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgCXUELHRf2E"
      },
      "source": [
        "## Testing dataset preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fqy84Dmp3bYa"
      },
      "source": [
        "\n",
        "Let's extract word pairs from the file `wordsim_similarity_goldstandard.txt` and compute the vector cosine similarity in each model. Compute the correlation between the similarity estimators of the google-news-vectors model model and human ratings of the wordsim dataset, and then - the similarity correlation between the model based on British national corpus and a human ratings of the wordsim dataset. Which model is closer to the human ratings?\n",
        "\n",
        "(use only such words from wordsim dataset, which have the corresponding vectors in the British national corpus labeled as NOUNs!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "Bpeg6FQd3clf",
        "outputId": "a225bc62-c220-44d1-9335-571b2e8ad540"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"wordsim353_sim_rel/wordsim_similarity_goldstandard.txt\",\n",
        "                 sep=\"\\t\", header=None)\n",
        "df.columns = [\"first\", \"second\", \"score\"]\n",
        "df.head(3)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   first second  score\n",
              "0  tiger    cat   7.35\n",
              "1  tiger  tiger  10.00\n",
              "2  plane    car   5.77"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-59bff160-d468-43cb-9fdb-39d3700b3eba\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>first</th>\n",
              "      <th>second</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tiger</td>\n",
              "      <td>cat</td>\n",
              "      <td>7.35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tiger</td>\n",
              "      <td>tiger</td>\n",
              "      <td>10.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>plane</td>\n",
              "      <td>car</td>\n",
              "      <td>5.77</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-59bff160-d468-43cb-9fdb-39d3700b3eba')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-59bff160-d468-43cb-9fdb-39d3700b3eba button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-59bff160-d468-43cb-9fdb-39d3700b3eba');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6b7c8b9a-816e-46da-bda8-bf2bed7ec7c2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6b7c8b9a-816e-46da-bda8-bf2bed7ec7c2')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6b7c8b9a-816e-46da-bda8-bf2bed7ec7c2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 203,\n  \"fields\": [\n    {\n      \"column\": \"first\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 130,\n        \"samples\": [\n          \"marathon\",\n          \"century\",\n          \"vodka\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"second\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 177,\n        \"samples\": [\n          \"Jackson\",\n          \"fauna\",\n          \"interview\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.503961087679943,\n        \"min\": 0.23,\n        \"max\": 10.0,\n        \"num_unique_values\": 150,\n        \"samples\": [\n          8.34,\n          6.63,\n          3.04\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDcXFGZnRf2e"
      },
      "source": [
        "## Model similarity evaluation\n",
        "We use only such words from wordsim dataset, which have the corresponding vectors in the British national corpus labeled as nouns, make 3 sets with similarity measures:\n",
        "\n",
        "1. Measures (cosine between vectors), obtained for the google-news-vectors model\n",
        "\n",
        "2. Measures (cosine between vectors), obtained for the model based on the British national corpus\n",
        "\n",
        "3. Human ratings from word_sim for the words, having the corresponding vectors in the British national corpus\n",
        "\n",
        "The skipped words from word_sim are presented in the outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qry_vLEd9758",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d3dbb65-9c0d-4920-e453-3f979b0b3994"
      },
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "gn_dist, br_dist, scores = [], [], []\n",
        "\n",
        "for row in df.iterrows():\n",
        "\n",
        "    w1, w2 = row[1][\"first\"], row[1][\"second\"]\n",
        "    try:\n",
        "        #enter your code here\n",
        "        br_dist.append(cosine(w_british[w1 + '_NOUN'], w_british[w2 + '_NOUN']))\n",
        "\n",
        "        gn_dist.append(cosine(w[w1], w[w2]))\n",
        "\n",
        "        scores.append(row[1][\"score\"])\n",
        "\n",
        "    except KeyError as e:\n",
        "        print(e, \"Skipping this word.\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Key 'stupid_NOUN' not present\" Skipping this word.\n",
            "\"Key 'Arafat_NOUN' not present\" Skipping this word.\n",
            "\"Key 'Japanese_NOUN' not present\" Skipping this word.\n",
            "\"Key 'Harvard_NOUN' not present\" Skipping this word.\n",
            "\"Key 'Mexico_NOUN' not present\" Skipping this word.\n",
            "\"Key 'live_NOUN' not present\" Skipping this word.\n",
            "\"Key 'seven_NOUN' not present\" Skipping this word.\n",
            "\"Key 'five_NOUN' not present\" Skipping this word.\n",
            "\"Key 'Mars_NOUN' not present\" Skipping this word.\n",
            "\"Key 'Wednesday_NOUN' not present\" Skipping this word.\n",
            "\"Key 'CD_NOUN' not present\" Skipping this word.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPyeIR2QtSec"
      },
      "source": [
        "## Model selection: correlation with human ratings\n",
        "\n",
        "Compute Spearman's correlation between each model and human ratings from word_sim.\n",
        "\n",
        "The results are in the outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZlbnwcq-SCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a928b7a-44ac-417d-c10c-3ce1f9661867"
      },
      "source": [
        "from scipy.stats import spearmanr\n",
        "\n",
        "coef_gn, p = spearmanr(gn_dist, scores)\n",
        "print(\"gn  Spearman R: \", coef_gn)\n",
        "\n",
        "coef_br, p = spearmanr(br_dist, scores)\n",
        "print(\"br Spearman R: \", coef_br)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gn  Spearman R:  -0.7842376265134542\n",
            "br Spearman R:  -0.7644278401221205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert  coef_gn == 0.7817164245392593\n",
        "assert  coef_br == 0.7627551934489611"
      ],
      "metadata": {
        "id": "uPktPSsLDKF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtlAncsQANfx"
      },
      "source": [
        "You can notice, that the google-news-vectors model is slighly better in this case."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Individual task\n",
        "\n",
        "1. Compute the cosine distance between the words vectors: `student` and `smart`\n",
        "    * Enter the result obtained for the GN model\n",
        "    * Enter the result obtained for the BR model"
      ],
      "metadata": {
        "id": "RU6QfMOIaRwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'GN model: {round(cosine(w[\"student\"], w[\"smart\"]), 3)}')\n",
        "print(f'BR model: {round(cosine(w_british[\"student_NOUN\"], w_british[\"smart_NOUN\"]), 3)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCzFDIWqaoxu",
        "outputId": "5a56c332-6e35-4eaa-d517-05808786f862"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GN model: 0.934\n",
            "BR model: 0.72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. For the given set of words `student smart wood money`, find an odd-one-out word.\n",
        "    * Enter the result obtained for the GN model:\n",
        "    * Enter the result obtained for the BR model"
      ],
      "metadata": {
        "id": "v45-RXK4aoI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"GN model odd-one-out word: {w.doesnt_match('student smart wood money'.split(' '))}\")\n",
        "print(f\"BR model odd-one-out word: {w_british.doesnt_match('student_NOUN smart_NOUN wood_NOUN money_NOUN'.split(' '))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvUMRDwsa2Km",
        "outputId": "ed28c35a-04f7-49c4-d76d-eb9b3f33dcad"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GN model odd-one-out word: wood\n",
            "BR model odd-one-out word: student_NOUN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Find the cosine distance between sentence vectors:\n",
        "\n",
        "*Disclaimer: the words missing in the GN model are deleted from the original proverbs.*\n",
        "\n",
        "`journey thousand miles begins with single step`\n",
        "\n",
        "&\n",
        "\n",
        "`leopard can not change its spots`\n",
        "\n",
        "To build the sentence vector, build the vector for each word from the sentence, and then average these vectors components-wise (we recommend to use `numpy.mean()` with acorrect parameters `axis`).\n",
        "    \n",
        "* Enter the result obtained for the GN model:"
      ],
      "metadata": {
        "id": "ZhYe8zlba2We"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_1 = \"journey thousand miles begins with single step\".split()\n",
        "text_2 = \"leopard can not change its spots\".split()\n",
        "\n",
        "v1 = np.mean([w[word] for word in text_1], axis=0)\n",
        "v2 = np.mean([w[word] for word in text_2], axis=0)\n",
        "\n",
        "round(cosine(v1, v2), 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxHGfMGtbGRT",
        "outputId": "9e9b590c-913f-4ebc-a3f5-664bb79d72d4"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.672"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Select the word pair set with `19:119` indices from the word_sim word set (the numbering starts from 0, the right boundary is not included).\n",
        "\n",
        "    Use only such pairs, which have the corresponding vectors in the British national corpus, labeled as nouns! Otherwise delete such a pair from the subset.\n",
        "\n",
        "    Compute Spearman's correlation between the similarity measures of the selected wordpairs, obtained as results of models running, and the human ratings in the word_sim dataset.\n",
        "\n",
        "\n",
        "* Enter the Spearman's correlation coefficient obtained for the GN model\n",
        "* Enter the Spearman's correlation coefficient obtained for the BR model\n",
        "* Enter the number of the skipped from the subset wordpairs"
      ],
      "metadata": {
        "id": "AfVDF6CrbHCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df.iloc[19:119]\n",
        "gn_dist, br_dist, scores = [], [], []\n",
        "num_skip = 0\n",
        "\n",
        "for row in df1.iterrows():\n",
        "\n",
        "    w1, w2 = row[1][\"first\"], row[1][\"second\"]\n",
        "    try:\n",
        "        #enter your code here\n",
        "        br_dist.append(cosine(w_british[w1 + '_NOUN'], w_british[w2 + '_NOUN']))\n",
        "\n",
        "        gn_dist.append(cosine(w[w1], w[w2]))\n",
        "\n",
        "        scores.append(row[1][\"score\"])\n",
        "\n",
        "    except KeyError as e:\n",
        "        num_skip += 1\n",
        "        print(e, \"Skipping this word.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3kF2M2obdRy",
        "outputId": "6d62615e-f052-406c-8e47-329caea5e00a"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Key 'Arafat_NOUN' not present\" Skipping this word.\n",
            "\"Key 'Japanese_NOUN' not present\" Skipping this word.\n",
            "\"Key 'Harvard_NOUN' not present\" Skipping this word.\n",
            "\"Key 'Mexico_NOUN' not present\" Skipping this word.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "coef_gn, p = spearmanr(gn_dist, scores)\n",
        "print(\"GN Spearman R: \", round(coef_gn, 3))\n",
        "\n",
        "coef_br, p = spearmanr(br_dist, scores)\n",
        "print(\"BR Spearman R: \", round(coef_br, 3))\n",
        "\n",
        "print(\"Number of the skipped = \", num_skip)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d60O1rr3eTC1",
        "outputId": "7c3c7817-9e3f-4c6a-8a6d-b1f22a0f3225"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GN Spearman R:  -0.703\n",
            "BR Spearman R:  -0.663\n",
            "Number of the skipped =  4\n"
          ]
        }
      ]
    }
  ]
}